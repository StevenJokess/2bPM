{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可解释性\n",
    "\n",
    "在可解释性瓶颈方面，龚克称，随着人工智能的快速应用，人们对可信任的人工智能呼声越来越高，而要使人工智能性能可信任，首先就要让它做到可解释。如果在可解释方面取得突破，那么就可以突破它的可靠性问题等。\n",
    "\n",
    "　　在过去一年，业内在可解释领域已有进展。比如，从几何角度来理解深度学习，提出生成对抗网络 (GANs)的最优传输(OT) 观点。“它通过几何的映射找到了生成对抗网络中的生成器和判决器之间的关系，进而找到了模型坍塌的原因，并提出了一个改进模型，这不能说是在解决可解释瓶颈上获得了突破，但是是一个非常有意义的进展。”龚克说道。\n",
    "\n",
    "　　再如，2020年7月，柏林技术大学和康奈尔大学的研究团队发表了公平无偏的排序学习模型FairCo，该研究分析了当前排序模型普遍存在的位置偏差、排序公平性以及物品曝光的马太效应问题等，基于反事实学习技术提出了具有公平性约束的相关度无偏估计方法，并实现了排序性能的提升，受到了业界广泛关注。\n",
    "\n",
    "　　但龚克强调，尽管已有上述成果，然而，无论是深度学习体系的创新，还是多种学习方式的融合创新，以及对已有算法进行解释的研究进展，具备理解能力的算法模型目前尚未显现。他称，“我们必须强烈呼吁，把可解释性作为下一段AI领域基础研究的主攻方向，争取在不久的将来能够为AI的进一步广泛应用提供坚实的基础。”[^1]\n",
    "\n",
    "[^1]: http://finance.eastmoney.com/a/202107081990173959.html"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}