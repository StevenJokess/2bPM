{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "nearby-bosnia",
   "metadata": {},
   "source": [
    "# 模型过程\n",
    "\n",
    "## 生命周期 [1]\n",
    "\n",
    "![模型的生命周期](../img/ML_Lifecycle.png)\n",
    "\n",
    "## 模型开发\n",
    "\n",
    "数据收集-》数据清理&可视化\n",
    "\n",
    "需要数据工程师来帮助：\n",
    "\n",
    "- 识别数据的潜在来源\n",
    "- 从多个来源连接数据\n",
    "- 定位缺失值和异常值\n",
    "- 绘制趋势以识别异常\n",
    "\n",
    "—》特征工程&模型设计-》训练&评估\n",
    "\n",
    "数据科学家为此而生：\n",
    "\n",
    "- 构建信息丰富的特性\n",
    "- 功能设计新的模型架构\n",
    "- 重新调整超参数\n",
    "- 验证预测准确性\n",
    "\n",
    "### 特征和特征工程\n",
    "\n",
    "特征:输入的属性或特征\n",
    "\n",
    "为什么从模型开发中直接生成训练有素的模型是一个坏主意\n",
    "\n",
    "为了解决新问题，正在构建模型\n",
    "\n",
    "- 需要跟踪组合和验证端到端的准确性。\n",
    "- 需要对模型进行单元和集成测试\n",
    "\n",
    "TODO:\n",
    "\n",
    "Dynamic Features: features can often be modified faster than models：\n",
    "\n",
    "- Useful for addressing fast changing dynamics (e.g., user preferences can be encoded in click history features).\n",
    "- Issue:resulting potential covariate shift can be problematic\n",
    "\n",
    "\n",
    "\n",
    "### 超参数\n",
    "\n",
    "参数和更普遍的配置细节不是通过培训直接确定的\n",
    "\n",
    "- 手动设置或使用交叉验证进行调整\n",
    "- 为什么不直接学习?\n",
    "\n",
    "![寻找超参](../img/find_hyperparameter.png)\n",
    "\n",
    "##\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "Training Pipelines -> Code\n",
    "Trained Models -> Binaries\n",
    "\n",
    "### 训练科技\n",
    "\n",
    "![训练科技](../img/train_tech.png)\n",
    "\n",
    "## 推断\n",
    "\n",
    "目的:在深度神经复杂的突发运行下在~10ms内进行预测\n",
    "\n",
    "### 并包括反馈\n",
    "\n",
    "1. 模型更新:当新数据来重新培训。\n",
    "\n",
    "- **周期性**:权衡利用批处理和验证。因为模型可能会过期一段时间。\n",
    "- **持续地(在线学习)**:最新鲜的模型。需要验证，学习率?…非常复杂。\n",
    "\n",
    "2. 特征更新:新数据可能会改变特征\n",
    "\n",
    "- 例如:更新用户的点击历史-》新预测\n",
    "- 比在线学习更健壮\n",
    "\n",
    "\n",
    "新的训练数据到达并改变损失面。\n",
    "而不是从以前的解的随机权重开始重新开始\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 反馈圈\n",
    "\n",
    "Models can bias the data they collect\n",
    "\n",
    "- Example: content recommendation\n",
    "- Future models may reflect earlier model bias\n",
    "\n",
    "Exploration –Exploitation Trade-off\n",
    "\n",
    "- Exploration:observe diverse outcomes\n",
    "- Exploitation:leverage model to takepredicted best action\n",
    "\n",
    "Solutions：\n",
    "\n",
    "- Randomization (ε-greedy): occasionally ignore the model\n",
    "- Bandit Algorithms/Thompson Sampling: optimally balance exploration and exploitation àactive area of research\n",
    "\n",
    "\n",
    "[1]: https://ucbrise.github.io/cs294-ai-sys-fa19/assets/lectures/lec03/03_ml-lifecycle.pdf"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}